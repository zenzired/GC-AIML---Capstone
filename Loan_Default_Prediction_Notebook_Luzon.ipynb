{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1a07ac",
   "metadata": {},
   "source": [
    "# Loan Default Prediction using Deep Learning\n",
    "In this notebook, we will:\n",
    "1. Perform data preprocessing and feature transformation.\n",
    "2. Conduct exploratory data analysis (EDA).\n",
    "3. Perform additional feature engineering.\n",
    "4. Build a deep learning model using Keras.\n",
    "5. Model Evaluation\n",
    "6. Use hyperparameter tuning and cross-validation to optimize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dfff2",
   "metadata": {},
   "source": [
    "### Step 1: Importing Libraries and Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50733660-ae70-4864-a052-da88980e13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\louie\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\louie\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85edfbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, RandomizedSearchCV, StratifiedKFold\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Step 1: Importing Libraries and Loading Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings # - v1.1\n",
    "warnings.filterwarnings('ignore') # - v1.1\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cbbd0",
   "metadata": {},
   "source": [
    "### Step 2: EDA - Data Preprocessing and Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.0: Data Preprocessing and Feature Transformation\n",
    "# Transform categorical values into numerical values (discrete)\n",
    "label_encoder = LabelEncoder()\n",
    "df['purpose'] = label_encoder.fit_transform(df['purpose'])\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Exploratory Data Analysis (EDA)\n",
    "# Visualize class imbalance\n",
    "sns.countplot(x='credit.policy', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5668e",
   "metadata": {},
   "source": [
    "### Step 3: Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f142cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Additional Feature Engineering\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['installment', 'log.annual.inc', 'dti', 'fico', 'days.with.cr.line', 'revol.bal', 'revol.util', 'inq.last.6mths', 'delinq.2yrs', 'pub.rec']\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('credit.policy', axis=1)\n",
    "y = df['credit.policy']\n",
    "\n",
    "# Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247c042",
   "metadata": {},
   "source": [
    "### Step 4: Build the Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be01253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build the Deep Learning Model\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), activation=activation))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the KerasClassifier\n",
    "model = KerasClassifier(model=create_model, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c8960",
   "metadata": {},
   "source": [
    "### Step 5: Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner\n",
    "# !pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02bf1af",
   "metadata": {},
   "source": [
    "### Step 6: Model Tuning, Optimization Hyperparameter Tuning and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77768f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: Hyperparameter Tuning\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Function to create model, with hyperparameters as arguments\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    # Use an Input layer\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap Keras model with KerasClassifier using scikeras\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters, using 'model__' prefix for model-specific parameters\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'model__activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with Cross-Validation\n",
    "print(\"Starting Randomized Search for Hyperparameter Tuning...\")\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=2)\n",
    "random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = random_search_result.best_params_\n",
    "best_model = random_search_result.best_estimator_\n",
    "\n",
    "print(\"Hyperparameter Tuning Complete!\")\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b15ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Hyperparameter Tuning\n",
    "# Define the grid of hyperparameters, using 'model__' prefix for model-specific parameters\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'model__activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with Cross-Validation\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)\n",
    "random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = random_search_result.best_params_\n",
    "best_model = random_search_result.best_estimator_\n",
    "\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Cross-Validation\n",
    "# Convert to numpy arrays if not already\n",
    "X_train_array = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "y_train_array = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "# Define StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X_train_array, y_train_array):\n",
    "    history = model.fit(X_train_array[train_idx], y_train_array[train_idx])\n",
    "    score = model.score(X_train_array[test_idx], y_train_array[test_idx])\n",
    "    results.append(score)\n",
    "\n",
    "print(f'Cross-Validation Scores: {results}')\n",
    "print(f'Mean Accuracy: {np.mean(results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84017eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary and Conclusion\n",
    "In this project, a deep learning model to predict loan defaults using historical data. The steps involved data preprocessing, exploratory data analysis, feature engineering, model building, hyperparameter tuning, and cross-validation. \n",
    "1. Data Preprocessing: Encoding categorical variables and scaling numerical features to prepare the data for modeling.\n",
    "2. Exploratory Data Analysis: Understanding the data distribution and relationships between features.\n",
    "3. Feature Engineering: Ensuring that all features are scaled appropriately for model training.\n",
    "4. Model Building: Using Keras to build a neural network capable of capturing complex patterns in the data.\n",
    "5. Hyperparameter Tuning: Optimizing the model’s hyperparameters to improve its performance.\n",
    "6. Cross-Validation: Validating the mode\n",
    "\n",
    "Following these steps, a deep learning model that can predict the likelihood of a loan default. The use of hyperparameter tuning and cross-validation ensures that the model is well-optimized and generalizes effectively. This approach provides a foundation for building predictive models in financial domains, where accurate risk assessment is crucial.\n",
    "\n",
    "Implementing a loan default prediction model in a real-world scenario involves more than just building this model. It requires planning around data collection, deployment, integration, monitoring, and compliance. By automating the data pipeline and decision-making processes, and incorporating continuous monitoring and retraining, the model can provide significant value to financial institutions in managing risk and making informed lending decisions. This approach can help streamline the loan approval process, reduce default rates, and ultimately improve the profitability and efficiency of the lending operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
